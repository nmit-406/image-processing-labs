{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLoss(x,y,a):\n",
    "    loss=0\n",
    "    for i in range(0,4):\n",
    "        loss+=(x[i]*a-y[i])**2\n",
    "    return loss/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeGrad(x,y,a):\n",
    "    grad=0\n",
    "    grad+=(2*x*x*a-2*x*y)/4\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 84431.25\n",
      "0 a: 6\n",
      "1 loss: 73025.23573143996\n",
      "1 a: 5.66021703235\n",
      "2 loss: 63161.31575859357\n",
      "2 a: 5.344227227869097\n",
      "3 loss: 54631.03918917242\n",
      "3 a: 5.050364480049638\n",
      "4 loss: 47254.12942104765\n",
      "4 a: 4.777079350809901\n",
      "5 loss: 40874.67629410868\n",
      "5 a: 4.522930900835343\n",
      "6 loss: 35357.842871477274\n",
      "6 a: 4.28657909199686\n",
      "7 loss: 30587.017299069157\n",
      "7 a: 4.066777721786594\n",
      "8 loss: 26461.349592090468\n",
      "8 a: 3.8623678525169858\n",
      "9 loss: 22893.621326331566\n",
      "9 a: 3.6722717006374594\n",
      "10 loss: 19808.403242754313\n",
      "10 a: 3.49548695394922\n",
      "11 loss: 17140.46185433717\n",
      "11 a: 3.3310814867547465\n",
      "12 loss: 14833.381402858344\n",
      "12 a: 3.1781884450767843\n",
      "13 loss: 12838.372061315175\n",
      "13 a: 3.03600167603286\n",
      "14 loss: 11113.239211047934\n",
      "14 a: 2.903771477265977\n",
      "15 loss: 9621.492024424158\n",
      "15 a: 2.780800644019685\n",
      "16 loss: 8331.572525987533\n",
      "16 a: 2.6664407930151\n",
      "17 loss: 7216.1888494152845\n",
      "17 a: 2.5600889437469307\n",
      "18 loss: 6251.7386081948625\n",
      "18 a: 2.461184339172849\n",
      "19 loss: 5417.8102010979255\n",
      "19 a: 2.369205489032787\n",
      "20 loss: 4696.751519489741\n",
      "20 a: 2.2836674202085874\n",
      "21 loss: 4073.2969470253547\n",
      "21 a: 2.204119119626096\n",
      "22 loss: 3534.244773413534\n",
      "22 a: 2.130141156216989\n",
      "23 loss: 3068.1782086753133\n",
      "23 a: 2.061343469401744\n",
      "24 loss: 2665.2241051723104\n",
      "24 a: 1.9973633124331922\n",
      "25 loss: 2316.844291077743\n",
      "25 a: 1.9378633397565883\n",
      "26 loss: 2015.6551077299396\n",
      "26 a: 1.882529828301518\n",
      "27 loss: 1755.2713389893474\n",
      "27 a: 1.8310710233271024\n",
      "28 loss: 1530.1712358964603\n",
      "28 a: 1.78321560009872\n",
      "29 loss: 1335.579785479275\n",
      "29 a: 1.7387112332851842\n",
      "30 loss: 1167.3677578946367\n",
      "30 a: 1.6973232665332976\n",
      "31 loss: 1021.9643993460897\n",
      "31 a: 1.658833475204904\n",
      "32 loss: 896.2819264396829\n",
      "32 a: 1.6230389157527718\n",
      "33 loss: 787.6502269055761\n",
      "33 a: 1.589750855668459\n",
      "34 loss: 693.7603871915817\n",
      "34 a: 1.5587937783601415\n",
      "35 loss: 612.6158538777715\n",
      "35 a: 1.5300044577134642\n",
      "36 loss: 542.4901971066071\n",
      "36 a: 1.5032310974558887\n",
      "37 loss: 481.89058367606856\n",
      "37 a: 1.4783325307867006\n",
      "38 loss: 429.5261880491886\n",
      "38 a: 1.4551774760525984\n",
      "39 loss: 384.2808738393497\n",
      "39 a: 1.433643844544293\n",
      "40 loss: 345.189568539634\n",
      "40 a: 1.4136180967643681\n",
      "41 loss: 311.4178322815766\n",
      "41 a: 1.3949946437722203\n",
      "42 loss: 282.24418888159573\n",
      "42 a: 1.3776752904495724\n",
      "43 loss: 257.0448457871976\n",
      "43 a: 1.361568717751095\n",
      "44 loss: 235.28048000217916\n",
      "44 a: 1.3465900012102119\n",
      "45 loss: 216.484810716421\n",
      "45 a: 1.3326601631613433\n",
      "46 loss: 200.2547171132182\n",
      "46 a: 1.3197057563175998\n",
      "47 loss: 186.24169247277814\n",
      "47 a: 1.30765847650828\n",
      "48 loss: 174.14445392393517\n",
      "48 a: 1.2964548025342655\n",
      "49 loss: 163.7025516136621\n",
      "49 a: 1.2860356612423944\n",
      "50 loss: 154.6908421813027\n",
      "50 a: 1.2763461160528644\n",
      "51 loss: 146.91470968762377\n",
      "51 a: 1.267335077297378\n",
      "52 loss: 140.20593294354978\n",
      "52 a: 1.258955032840738\n",
      "53 loss: 134.41911184340623\n",
      "53 a: 1.251161797565559\n",
      "54 loss: 129.42857712116555\n",
      "54 a: 1.2439142803992067\n",
      "55 loss: 125.12571816512742\n",
      "55 a: 1.2371742676545814\n",
      "56 loss: 121.4166723624441\n",
      "56 a: 1.2309062215423736\n",
      "57 loss: 118.22032708658926\n",
      "57 a: 1.2250770927924184\n",
      "58 loss: 115.46659204965624\n",
      "58 a: 1.2196561463961595\n",
      "59 loss: 113.09490545689025\n",
      "59 a: 1.2146147995514298\n",
      "60 loss: 111.05294234380875\n",
      "60 a: 1.2099264709550783\n",
      "61 loss: 109.2954977510862\n",
      "61 a: 1.2055664406488236\n",
      "62 loss: 107.7835210893653\n",
      "62 a: 1.2015117196793386\n",
      "63 loss: 106.48328124341076\n",
      "63 a: 1.1977409288853398\n",
      "64 loss: 105.36564473008403\n",
      "64 a: 1.1942341861725578\n",
      "65 loss: 104.40545161590731\n",
      "65 a: 1.1909730016822329\n",
      "66 loss: 103.58097596801991\n",
      "66 a: 1.1879401803003926\n",
      "67 loss: 102.87345940080245\n",
      "67 a: 1.1851197309938803\n",
      "68 loss: 102.2667078271424\n",
      "68 a: 1.182496782495087\n",
      "69 loss: 101.74674286091872\n",
      "69 a: 1.1800575048908284\n",
      "70 loss: 101.30150047404891\n",
      "70 a: 1.1777890367019277\n",
      "71 loss: 100.92057051182782\n",
      "71 a: 1.1756794170690203\n",
      "72 loss: 100.59497153541469\n",
      "72 a: 1.1737175226870216\n",
      "73 loss: 100.31695620848862\n",
      "73 a: 1.1718930091557291\n",
      "74 loss: 100.0798430921007\n",
      "74 a: 1.1701962564373305\n",
      "75 loss: 99.87787127127012\n",
      "75 a: 1.1686183181332201\n",
      "76 loss: 99.70607472074423\n",
      "76 a: 1.1671508743126917\n",
      "77 loss: 99.56017373577416\n",
      "77 a: 1.16578618764478\n",
      "78 loss: 99.43648111561606\n",
      "78 a: 1.1645170626019516\n",
      "79 loss: 99.33182110038234\n",
      "79 a: 1.1633368075205426\n",
      "80 loss: 99.24345933246991\n",
      "80 a: 1.1622391993178967\n",
      "81 loss: 99.1690423477907\n",
      "81 a: 1.1612184506801722\n",
      "82 loss: 99.10654530438634\n",
      "82 a: 1.1602691795478093\n",
      "83 loss: 99.05422683098368\n",
      "83 a: 1.1593863807377651\n",
      "84 loss: 99.01059002936356\n",
      "84 a: 1.1585653995528895\n",
      "85 loss: 98.97434879525483\n",
      "85 a: 1.1578019072392942\n",
      "86 loss: 98.94439873560418\n",
      "86 a: 1.1570918781623096\n",
      "87 loss: 98.91979205790616\n",
      "87 a: 1.1564315685806852\n",
      "88 loss: 98.89971589186817\n",
      "88 a: 1.155817496907118\n",
      "89 loss: 98.883473576834\n",
      "89 a: 1.155246425351031\n",
      "90 loss: 98.87046851163296\n",
      "90 a: 1.1547153428468062\n",
      "91 loss: 98.86019021820749\n",
      "91 a: 1.1542214491774623\n",
      "92 loss: 98.85220231765643\n",
      "92 a: 1.1537621402100653\n",
      "93 loss: 98.84613215820815\n",
      "93 a: 1.1533349941650242\n",
      "94 loss: 98.8416618699872\n",
      "94 a: 1.1529377588468712\n",
      "95 loss: 98.83852065199414\n",
      "95 a: 1.1525683397692048\n",
      "96 loss: 98.836478123138\n",
      "96 a: 1.1522247891111754\n",
      "97 loss: 98.83533859200298\n",
      "97 a: 1.151905295447291\n",
      "98 loss: 98.83493611977292\n",
      "98 a: 1.1516081741963882\n",
      "99 loss: 98.83513026781166\n",
      "99 a: 1.151331858739409\n"
     ]
    }
   ],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "data = np.genfromtxt('/home/dio/image-processing-labs/linear-regression,10-1/data.csv',delimiter=',')\n",
    "for i in data:\n",
    "    x.append(i[0])\n",
    "    y.append(i[1])\n",
    "a=random.randint(1,100)\n",
    "epoch=100\n",
    "learning_rate=0.00001\n",
    "for i in range(0,epoch):\n",
    "    print(str(i)+\" loss: \"+str(computeLoss(x,y,a)))\n",
    "    print(str(i)+\" a: \"+str(a))\n",
    "    for j in range(0,4):\n",
    "        a=a-learning_rate*computeGrad(x[j],y[j],a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
