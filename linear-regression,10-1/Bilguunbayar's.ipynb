{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLoss(x,y,a):\n",
    "    loss=0\n",
    "    for i in range(0,4):\n",
    "        loss+=(x[i]*a-y[i])**2\n",
    "    return loss/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeGrad(x,y,a):\n",
    "    grad=0\n",
    "    grad+=(2*x*x*a-2*x*y)/4\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 14624981.25\n",
      "0 a: 65\n",
      "1 loss: 2833951.530076554\n",
      "1 a: 29.2571562890625\n",
      "2 loss: 548042.4287904219\n",
      "2 a: 13.510244664458202\n",
      "3 loss: 105533.25472127854\n",
      "3 a: 6.5727657185057495\n",
      "4 loss: 20161.363896608636\n",
      "4 a: 3.516381376427309\n",
      "5 loss: 3818.758112726859\n",
      "5 a: 2.169856894078806\n",
      "6 loss: 747.0175690816017\n",
      "6 a: 1.5766303954994694\n",
      "7 loss: 194.98321063997858\n",
      "7 a: 1.3152777923868155\n",
      "8 loss: 107.29731412658671\n",
      "8 a: 1.2001359649105683\n",
      "9 loss: 98.85165275027768\n",
      "9 a: 1.1494089383698938\n",
      "10 loss: 100.98962253398567\n",
      "10 a: 1.1270605765446562\n",
      "11 loss: 103.06868846930155\n",
      "11 a: 1.1172147544980737\n",
      "12 loss: 104.20536037645181\n",
      "12 a: 1.1128770669068742\n",
      "13 loss: 104.74897366807079\n",
      "13 a: 1.1109660498734741\n",
      "14 loss: 104.99678355356762\n",
      "14 a: 1.1101241298353979\n",
      "15 loss: 105.1075728659466\n",
      "15 a: 1.1097532124976004\n",
      "16 loss: 105.15669558806131\n",
      "16 a: 1.1095898006945115\n",
      "17 loss: 105.17839795458234\n",
      "17 a: 1.1095178077875545\n",
      "18 loss: 105.18797097687792\n",
      "18 a: 1.1094860905030368\n",
      "19 loss: 105.19219076992789\n",
      "19 a: 1.1094721170967707\n",
      "20 loss: 105.19405029175991\n",
      "20 a: 1.1094659609555486\n",
      "21 loss: 105.19486961129552\n",
      "21 a: 1.1094632487983247\n",
      "22 loss: 105.19523058847568\n",
      "22 a: 1.1094620539269693\n",
      "23 loss: 105.19538962428122\n",
      "23 a: 1.1094615275129607\n",
      "24 loss: 105.1954596899241\n",
      "24 a: 1.1094612955953513\n",
      "25 loss: 105.19549055825333\n",
      "25 a: 1.1094611934214411\n",
      "26 loss: 105.19550415766658\n",
      "26 a: 1.1094611484075763\n",
      "27 loss: 105.19551014904566\n",
      "27 a: 1.109461128576212\n",
      "28 loss: 105.19551278861584\n",
      "28 a: 1.1094611198392814\n",
      "29 loss: 105.19551395150839\n",
      "29 a: 1.1094611159901286\n",
      "30 loss: 105.19551446383372\n",
      "30 a: 1.1094611142943411\n",
      "31 loss: 105.19551468954448\n",
      "31 a: 1.109461113547243\n",
      "32 loss: 105.19551478898389\n",
      "32 a: 1.1094611132181003\n",
      "33 loss: 105.19551483279307\n",
      "33 a: 1.1094611130730927\n",
      "34 loss: 105.19551485209368\n",
      "34 a: 1.1094611130092082\n",
      "35 loss: 105.1955148605967\n",
      "35 a: 1.1094611129810632\n",
      "36 loss: 105.19551486434278\n",
      "36 a: 1.1094611129686636\n",
      "37 loss: 105.19551486599335\n",
      "37 a: 1.1094611129632006\n",
      "38 loss: 105.19551486672044\n",
      "38 a: 1.1094611129607939\n",
      "39 loss: 105.19551486704071\n",
      "39 a: 1.1094611129597336\n",
      "40 loss: 105.19551486718188\n",
      "40 a: 1.1094611129592664\n",
      "41 loss: 105.19551486724401\n",
      "41 a: 1.1094611129590606\n",
      "42 loss: 105.19551486727141\n",
      "42 a: 1.10946111295897\n",
      "43 loss: 105.19551486728348\n",
      "43 a: 1.10946111295893\n",
      "44 loss: 105.19551486728884\n",
      "44 a: 1.1094611129589125\n",
      "45 loss: 105.1955148672911\n",
      "45 a: 1.109461112958905\n",
      "46 loss: 105.19551486729208\n",
      "46 a: 1.1094611129589016\n",
      "47 loss: 105.19551486729264\n",
      "47 a: 1.1094611129588998\n",
      "48 loss: 105.19551486729277\n",
      "48 a: 1.1094611129588992\n",
      "49 loss: 105.19551486729299\n",
      "49 a: 1.1094611129588987\n",
      "50 loss: 105.19551486729299\n",
      "50 a: 1.1094611129588987\n",
      "51 loss: 105.19551486729299\n",
      "51 a: 1.1094611129588987\n",
      "52 loss: 105.19551486729299\n",
      "52 a: 1.1094611129588987\n",
      "53 loss: 105.19551486729299\n",
      "53 a: 1.1094611129588987\n",
      "54 loss: 105.19551486729299\n",
      "54 a: 1.1094611129588987\n",
      "55 loss: 105.19551486729299\n",
      "55 a: 1.1094611129588987\n",
      "56 loss: 105.19551486729299\n",
      "56 a: 1.1094611129588987\n",
      "57 loss: 105.19551486729299\n",
      "57 a: 1.1094611129588987\n",
      "58 loss: 105.19551486729299\n",
      "58 a: 1.1094611129588987\n",
      "59 loss: 105.19551486729299\n",
      "59 a: 1.1094611129588987\n",
      "60 loss: 105.19551486729299\n",
      "60 a: 1.1094611129588987\n",
      "61 loss: 105.19551486729299\n",
      "61 a: 1.1094611129588987\n",
      "62 loss: 105.19551486729299\n",
      "62 a: 1.1094611129588987\n",
      "63 loss: 105.19551486729299\n",
      "63 a: 1.1094611129588987\n",
      "64 loss: 105.19551486729299\n",
      "64 a: 1.1094611129588987\n",
      "65 loss: 105.19551486729299\n",
      "65 a: 1.1094611129588987\n",
      "66 loss: 105.19551486729299\n",
      "66 a: 1.1094611129588987\n",
      "67 loss: 105.19551486729299\n",
      "67 a: 1.1094611129588987\n",
      "68 loss: 105.19551486729299\n",
      "68 a: 1.1094611129588987\n",
      "69 loss: 105.19551486729299\n",
      "69 a: 1.1094611129588987\n",
      "70 loss: 105.19551486729299\n",
      "70 a: 1.1094611129588987\n",
      "71 loss: 105.19551486729299\n",
      "71 a: 1.1094611129588987\n",
      "72 loss: 105.19551486729299\n",
      "72 a: 1.1094611129588987\n",
      "73 loss: 105.19551486729299\n",
      "73 a: 1.1094611129588987\n",
      "74 loss: 105.19551486729299\n",
      "74 a: 1.1094611129588987\n",
      "75 loss: 105.19551486729299\n",
      "75 a: 1.1094611129588987\n",
      "76 loss: 105.19551486729299\n",
      "76 a: 1.1094611129588987\n",
      "77 loss: 105.19551486729299\n",
      "77 a: 1.1094611129588987\n",
      "78 loss: 105.19551486729299\n",
      "78 a: 1.1094611129588987\n",
      "79 loss: 105.19551486729299\n",
      "79 a: 1.1094611129588987\n",
      "80 loss: 105.19551486729299\n",
      "80 a: 1.1094611129588987\n",
      "81 loss: 105.19551486729299\n",
      "81 a: 1.1094611129588987\n",
      "82 loss: 105.19551486729299\n",
      "82 a: 1.1094611129588987\n",
      "83 loss: 105.19551486729299\n",
      "83 a: 1.1094611129588987\n",
      "84 loss: 105.19551486729299\n",
      "84 a: 1.1094611129588987\n",
      "85 loss: 105.19551486729299\n",
      "85 a: 1.1094611129588987\n",
      "86 loss: 105.19551486729299\n",
      "86 a: 1.1094611129588987\n",
      "87 loss: 105.19551486729299\n",
      "87 a: 1.1094611129588987\n",
      "88 loss: 105.19551486729299\n",
      "88 a: 1.1094611129588987\n",
      "89 loss: 105.19551486729299\n",
      "89 a: 1.1094611129588987\n",
      "90 loss: 105.19551486729299\n",
      "90 a: 1.1094611129588987\n",
      "91 loss: 105.19551486729299\n",
      "91 a: 1.1094611129588987\n",
      "92 loss: 105.19551486729299\n",
      "92 a: 1.1094611129588987\n",
      "93 loss: 105.19551486729299\n",
      "93 a: 1.1094611129588987\n",
      "94 loss: 105.19551486729299\n",
      "94 a: 1.1094611129588987\n",
      "95 loss: 105.19551486729299\n",
      "95 a: 1.1094611129588987\n",
      "96 loss: 105.19551486729299\n",
      "96 a: 1.1094611129588987\n",
      "97 loss: 105.19551486729299\n",
      "97 a: 1.1094611129588987\n",
      "98 loss: 105.19551486729299\n",
      "98 a: 1.1094611129588987\n",
      "99 loss: 105.19551486729299\n",
      "99 a: 1.1094611129588987\n"
     ]
    }
   ],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "data = np.genfromtxt('/home/dio/image-processing-labs/linear-regression,10-1/data.csv',delimiter=',')\n",
    "for i in data:\n",
    "    x.append(i[0])\n",
    "    y.append(i[1])\n",
    "a=random.randint(1,100)\n",
    "epoch=100\n",
    "learning_rate=0.0001\n",
    "for i in range(0,epoch):\n",
    "    print(str(i)+\" loss: \"+str(computeLoss(x,y,a)))\n",
    "    print(str(i)+\" a: \"+str(a))\n",
    "    for j in range(0,4):\n",
    "        a=a-learning_rate*computeGrad(x[j],y[j],a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
