{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Admission_Predict_Ver1.1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0           1        337          118                  4  4.5   4.5  9.65   \n",
       "1           2        324          107                  4  4.0   4.5  8.87   \n",
       "2           3        316          104                  3  3.0   3.5  8.00   \n",
       "3           4        322          110                  3  3.5   2.5  8.67   \n",
       "4           5        314          103                  2  2.0   3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0         1              0.92  \n",
       "1         1              0.76  \n",
       "2         1              0.72  \n",
       "3         1              0.80  \n",
       "4         0              0.65  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sof=[]\n",
    "lor=[]\n",
    "gpa=[]\n",
    "y=[]\n",
    "data = np.genfromtxt('https://raw.githubusercontent.com/nmit-406/image-processing-labs/master/forward-propa%2C10-22/Admission_Predict_Ver1.1.csv',delimiter=',')\n",
    "check= True\n",
    "for i in data:\n",
    "    if(check):\n",
    "        check = False\n",
    "        continue\n",
    "\n",
    "    sof.append(i[4])\n",
    "    lor.append(i[5])\n",
    "    gpa.append(i[6])\n",
    "    y.append(i[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.5,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.5,\n",
       " 2.0,\n",
       " 4.5,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 3.5,\n",
       " 3.5,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.5,\n",
       " 3.5,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.5,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 4.5,\n",
       " 4.5,\n",
       " 1.5,\n",
       " 2.0,\n",
       " 1.5,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 3.5,\n",
       " 2.5,\n",
       " 2.5,\n",
       " 4.5,\n",
       " 4.5,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 4.5,\n",
       " 3.5,\n",
       " 3.0,\n",
       " 2.5,\n",
       " 1.5,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.5,\n",
       " 3.0,\n",
       " 3.5,\n",
       " 3.5,\n",
       " 4.5,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.5,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 1.5,\n",
       " 2.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 4.5,\n",
       " 4.5,\n",
       " 4.5,\n",
       " 3.5,\n",
       " 4.5,\n",
       " 4.5,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 1.5,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 3.5,\n",
       " 2.5,\n",
       " 4.0,\n",
       " 4.5,\n",
       " 3.5,\n",
       " 4.0,\n",
       " 4.5,\n",
       " 3.5,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 3.5,\n",
       " 4.0,\n",
       " 3.5,\n",
       " 4.5,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 5.0,\n",
       " 4.5,\n",
       " 1.5,\n",
       " 3.5,\n",
       " 2.5,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 2.5,\n",
       " 3.5,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 3.5,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.5,\n",
       " 5.0,\n",
       " 1.5,\n",
       " 4.5,\n",
       " 3.5,\n",
       " 4.0,\n",
       " 4.5,\n",
       " 4.0,\n",
       " 4.5,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 3.5,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 3.0,\n",
       " 3.5,\n",
       " 4.5,\n",
       " 4.5,\n",
       " 3.5,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.5,\n",
       " 2.5,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.5,\n",
       " 4.5,\n",
       " 3.5,\n",
       " 2.5,\n",
       " 3.0,\n",
       " 3.5,\n",
       " 2.5,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 2.5,\n",
       " 4.5,\n",
       " 3.5,\n",
       " 4.5,\n",
       " 4.5,\n",
       " 5.0,\n",
       " 4.5,\n",
       " 4.0,\n",
       " 4.5,\n",
       " 4.5,\n",
       " 3.5,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.5,\n",
       " 4.5,\n",
       " 4.0,\n",
       " 2.5,\n",
       " 3.5,\n",
       " 4.5,\n",
       " 4.0,\n",
       " 3.5,\n",
       " 2.5,\n",
       " 3.5,\n",
       " 3.5,\n",
       " 3.0,\n",
       " 3.5,\n",
       " 4.5,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.5,\n",
       " 5.0,\n",
       " 4.5,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.5,\n",
       " 4.0,\n",
       " 3.5,\n",
       " 4.5,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 2.5,\n",
       " 3.5,\n",
       " 3.5,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 3.5,\n",
       " 2.5,\n",
       " 2.5,\n",
       " 5.0,\n",
       " 4.5,\n",
       " 4.0,\n",
       " 4.5,\n",
       " 2.0,\n",
       " 1.5,\n",
       " 2.5,\n",
       " 2.5,\n",
       " 3.5,\n",
       " 3.5,\n",
       " 2.5,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 2.5,\n",
       " 3.5,\n",
       " 3.5,\n",
       " 3.0,\n",
       " 2.5,\n",
       " 2.5,\n",
       " 4.5,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.5,\n",
       " 2.5,\n",
       " 2.5,\n",
       " 3.0,\n",
       " 2.5,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 4.5,\n",
       " 4.5,\n",
       " 2.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 3.5,\n",
       " 5.0,\n",
       " 2.5,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 4.5,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 2.5,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 4.5,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 2.5,\n",
       " 1.5,\n",
       " 1.0,\n",
       " 3.5,\n",
       " 2.5,\n",
       " 3.0,\n",
       " 3.5,\n",
       " 4.0,\n",
       " 4.5,\n",
       " 3.0,\n",
       " 2.5,\n",
       " 2.5,\n",
       " 3.0,\n",
       " 3.5,\n",
       " 2.5,\n",
       " 3.5,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.5,\n",
       " 3.5,\n",
       " 3.0,\n",
       " 4.5,\n",
       " 4.5,\n",
       " 3.5,\n",
       " 3.0,\n",
       " 2.5,\n",
       " 1.5,\n",
       " 1.0,\n",
       " 2.5,\n",
       " 3.5,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 2.5,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 3.5,\n",
       " 2.0,\n",
       " 2.5,\n",
       " 4.0,\n",
       " 2.5,\n",
       " 3.5,\n",
       " 3.0,\n",
       " 3.5,\n",
       " 3.0,\n",
       " 4.5,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 3.5,\n",
       " 3.0,\n",
       " 3.5,\n",
       " 3.0,\n",
       " 2.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.5,\n",
       " 3.0,\n",
       " 3.5,\n",
       " 3.0,\n",
       " 3.5,\n",
       " 2.5,\n",
       " 2.0,\n",
       " 3.5,\n",
       " 3.5,\n",
       " 2.5,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.5,\n",
       " 2.5,\n",
       " 3.5,\n",
       " 4.5,\n",
       " 3.5,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.5,\n",
       " 3.5,\n",
       " 4.5,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.5,\n",
       " 1.5,\n",
       " 2.0,\n",
       " 2.5,\n",
       " 3.5,\n",
       " 3.0,\n",
       " 4.5,\n",
       " 3.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 2.5,\n",
       " 2.0,\n",
       " 1.5,\n",
       " 3.5,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 4.5,\n",
       " 3.5,\n",
       " 3.0,\n",
       " 5.0,\n",
       " 3.5,\n",
       " 5.0,\n",
       " 3.5,\n",
       " 3.0,\n",
       " 3.5,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 2.5,\n",
       " 3.0,\n",
       " 2.5,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.5,\n",
       " 2.5,\n",
       " 3.0,\n",
       " 3.5,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.5,\n",
       " 2.5,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 3.5,\n",
       " 4.5,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.5,\n",
       " 2.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 3.5,\n",
       " 4.5,\n",
       " 4.0,\n",
       " 3.5,\n",
       " 2.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 2.5,\n",
       " 1.5,\n",
       " 2.5,\n",
       " 1.5,\n",
       " 4.5,\n",
       " 4.5,\n",
       " 4.0,\n",
       " 4.5,\n",
       " 5.0,\n",
       " 3.5,\n",
       " 2.5,\n",
       " 3.5,\n",
       " 3.0,\n",
       " 4.5,\n",
       " 5.0,\n",
       " 2.5,\n",
       " 3.0,\n",
       " 1.5,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 2.5,\n",
       " 3.0,\n",
       " 3.5,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 3.5,\n",
       " 3.5,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.5,\n",
       " 2.5,\n",
       " 3.5,\n",
       " 4.0,\n",
       " 4.5,\n",
       " 3.5,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 3.5,\n",
       " 2.5,\n",
       " 2.5,\n",
       " 3.5,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 2.5,\n",
       " 3.0,\n",
       " 2.5,\n",
       " 3.0,\n",
       " 2.5,\n",
       " 4.5,\n",
       " 5.0,\n",
       " 4.5,\n",
       " 4.0,\n",
       " 4.5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLoss(sof,lor,gpa,y):\n",
    "    loss = 0\n",
    "    n= len(sof)\n",
    "    for j in range(n):\n",
    "        \n",
    "        ypre = (sof[j]*a+lor[j]*b+gpa[j]*c + d)*m + (sof[j]*e+lor[j]*f + gpa[j]*g + h)*k+l\n",
    "        loss+=(ypre-y[j])**2\n",
    "    return loss/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeGrad(sof,lor,gpa,y,ki):\n",
    "        n = len(sof)\n",
    "        i=0\n",
    "        for i in range(n):\n",
    "            y1 = 2*((sof[i]*a+lor[i]*b+gpa[i]*c + d)*m + (sof[i]*e+lor[i]*f + gpa[i]*g + h)*k+l-y[i])\n",
    "            grad=0\n",
    "            if(ki==\"a\"):\n",
    "                grad=y1*sof[i]*m\n",
    "            if(ki==\"b\"):\n",
    "                grad=y1*lor[i]*m\n",
    "            if(ki==\"c\"):\n",
    "                grad=y1*gpa[i]*m\n",
    "            if(ki==\"d\"):\n",
    "                grad=y1*m\n",
    "            if(ki==\"m\"):\n",
    "                grad=y1*(a*sof[i]+b*lor[i]+c*gpa[i]+d)\n",
    "            if(ki==\"e\"):\n",
    "                grad=y1*sof[i]*k\n",
    "            if(ki==\"f\"):\n",
    "                grad=y1*lor[i]*k\n",
    "            if(ki==\"g\"):\n",
    "                grad=y1*gpa[i]*k\n",
    "            if(ki==\"h\"):\n",
    "                grad=y1*k\n",
    "            if(ki==\"k\"):\n",
    "                grad=y1*(e*sof[i]+f*lor[i]+g*gpa[i]+h)\n",
    "            if(ki==\"l\"):\n",
    "                grad=y1\n",
    "        return grad/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= np.random.random()\n",
    "b= np.random.random()\n",
    "c= np.random.random()\n",
    "d= np.random.random()\n",
    "e= np.random.random()\n",
    "f= np.random.random()\n",
    "g= np.random.random()\n",
    "h= np.random.random()\n",
    "k= np.random.random()\n",
    "l= np.random.random()\n",
    "m= np.random.random()\n",
    "# arr= []\n",
    "# arr.append(a)\n",
    "# arr.append(b)\n",
    "# arr.append(c)\n",
    "# arr.append(d)\n",
    "# arr.append(e)\n",
    "# arr.append(f)\n",
    "# arr.append(g)\n",
    "# arr.append(h)\n",
    "# arr.append(k)\n",
    "# arr.append(l)\n",
    "# arr.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attemp 0: loss: 0.01612202855933232\n",
      "attemp 1: loss: 0.016116870835740236\n",
      "attemp 2: loss: 0.01611172383048437\n",
      "attemp 3: loss: 0.01610658751849597\n",
      "attemp 4: loss: 0.016101461874771315\n",
      "attemp 5: loss: 0.016096346874371444\n",
      "attemp 6: loss: 0.01609124249242196\n",
      "attemp 7: loss: 0.01608614870411282\n",
      "attemp 8: loss: 0.016081065484698284\n",
      "attemp 9: loss: 0.016075992809496648\n",
      "attemp 10: loss: 0.01607093065389\n",
      "attemp 11: loss: 0.01606587899332425\n",
      "attemp 12: loss: 0.016060837803308643\n",
      "attemp 13: loss: 0.016055807059415932\n",
      "attemp 14: loss: 0.016050786737281966\n",
      "attemp 15: loss: 0.01604577681260549\n",
      "attemp 16: loss: 0.01604077726114823\n",
      "attemp 17: loss: 0.016035788058734325\n",
      "attemp 18: loss: 0.016030809181250584\n",
      "attemp 19: loss: 0.016025840604645986\n",
      "attemp 20: loss: 0.016020882304931653\n",
      "attemp 21: loss: 0.016015934258180608\n",
      "attemp 22: loss: 0.016010996440527656\n",
      "attemp 23: loss: 0.016006068828169275\n",
      "attemp 24: loss: 0.016001151397363248\n",
      "attemp 25: loss: 0.015996244124428672\n",
      "attemp 26: loss: 0.015991346985745714\n",
      "attemp 27: loss: 0.01598645995775547\n",
      "attemp 28: loss: 0.015981583016959722\n",
      "attemp 29: loss: 0.01597671613992092\n",
      "attemp 30: loss: 0.015971859303261816\n",
      "attemp 31: loss: 0.015967012483665485\n",
      "attemp 32: loss: 0.015962175657875034\n",
      "attemp 33: loss: 0.01595734880269345\n",
      "attemp 34: loss: 0.015952531894983513\n",
      "attemp 35: loss: 0.015947724911667495\n",
      "attemp 36: loss: 0.015942927829727175\n",
      "attemp 37: loss: 0.01593814062620356\n",
      "attemp 38: loss: 0.015933363278196597\n",
      "attemp 39: loss: 0.01592859576286527\n",
      "attemp 40: loss: 0.01592383805742734\n",
      "attemp 41: loss: 0.015919090139159073\n",
      "attemp 42: loss: 0.015914351985395148\n",
      "attemp 43: loss: 0.015909623573528587\n",
      "attemp 44: loss: 0.015904904881010474\n",
      "attemp 45: loss: 0.015900195885349813\n",
      "attemp 46: loss: 0.01589549656411337\n",
      "attemp 47: loss: 0.015890806894925568\n",
      "attemp 48: loss: 0.015886126855468308\n",
      "attemp 49: loss: 0.01588145642348074\n",
      "attemp 50: loss: 0.015876795576759205\n",
      "attemp 51: loss: 0.01587214429315693\n",
      "attemp 52: loss: 0.015867502550584094\n",
      "attemp 53: loss: 0.01586287032700748\n",
      "attemp 54: loss: 0.015858247600450338\n",
      "attemp 55: loss: 0.01585363434899236\n",
      "attemp 56: loss: 0.015849030550769354\n",
      "attemp 57: loss: 0.015844436183973225\n",
      "attemp 58: loss: 0.015839851226851744\n",
      "attemp 59: loss: 0.015835275657708404\n",
      "attemp 60: loss: 0.01583070945490229\n",
      "attemp 61: loss: 0.015826152596847935\n",
      "attemp 62: loss: 0.015821605062015104\n",
      "attemp 63: loss: 0.01581706682892861\n",
      "attemp 64: loss: 0.01581253787616847\n",
      "attemp 65: loss: 0.01580801818236924\n",
      "attemp 66: loss: 0.015803507726220273\n",
      "attemp 67: loss: 0.015799006486465476\n",
      "attemp 68: loss: 0.015794514441902968\n",
      "attemp 69: loss: 0.01579003157138525\n",
      "attemp 70: loss: 0.01578555785381871\n",
      "attemp 71: loss: 0.01578109326816376\n",
      "attemp 72: loss: 0.015776637793434627\n",
      "attemp 73: loss: 0.015772191408699017\n",
      "attemp 74: loss: 0.01576775409307809\n",
      "attemp 75: loss: 0.01576332582574649\n",
      "attemp 76: loss: 0.015758906585931882\n",
      "attemp 77: loss: 0.01575449635291499\n",
      "attemp 78: loss: 0.015750095106029507\n",
      "attemp 79: loss: 0.015745702824661673\n",
      "attemp 80: loss: 0.015741319488250496\n",
      "attemp 81: loss: 0.01573694507628732\n",
      "attemp 82: loss: 0.01573257956831584\n",
      "attemp 83: loss: 0.015728222943931808\n",
      "attemp 84: loss: 0.01572387518278312\n",
      "attemp 85: loss: 0.01571953626456945\n",
      "attemp 86: loss: 0.015715206169042227\n",
      "attemp 87: loss: 0.01571088487600439\n",
      "attemp 88: loss: 0.01570657236531043\n",
      "attemp 89: loss: 0.015702268616866057\n",
      "attemp 90: loss: 0.015697973610628142\n",
      "attemp 91: loss: 0.015693687326604564\n",
      "attemp 92: loss: 0.0156894097448541\n",
      "attemp 93: loss: 0.01568514084548623\n",
      "attemp 94: loss: 0.015680880608661116\n",
      "attemp 95: loss: 0.015676629014589202\n",
      "attemp 96: loss: 0.015672386043531432\n",
      "attemp 97: loss: 0.015668151675798766\n",
      "attemp 98: loss: 0.015663925891752292\n",
      "attemp 99: loss: 0.01565970867180303\n",
      "attemp 100: loss: 0.015655499996411647\n",
      "attemp 101: loss: 0.015651299846088554\n",
      "attemp 102: loss: 0.015647108201393554\n",
      "attemp 103: loss: 0.015642925042935886\n",
      "attemp 104: loss: 0.015638750351373937\n",
      "attemp 105: loss: 0.015634584107415268\n",
      "attemp 106: loss: 0.01563042629181624\n",
      "attemp 107: loss: 0.015626276885382196\n",
      "attemp 108: loss: 0.015622135868967046\n",
      "attemp 109: loss: 0.015618003223473238\n",
      "attemp 110: loss: 0.015613878929851733\n",
      "attemp 111: loss: 0.015609762969101666\n",
      "attemp 112: loss: 0.015605655322270366\n",
      "attemp 113: loss: 0.015601555970453193\n",
      "attemp 114: loss: 0.015597464894793323\n",
      "attemp 115: loss: 0.015593382076481732\n",
      "attemp 116: loss: 0.015589307496757073\n",
      "attemp 117: loss: 0.015585241136905318\n",
      "attemp 118: loss: 0.015581182978259982\n",
      "attemp 119: loss: 0.015577133002201671\n",
      "attemp 120: loss: 0.015573091190158168\n",
      "attemp 121: loss: 0.015569057523604239\n",
      "attemp 122: loss: 0.01556503198406141\n",
      "attemp 123: loss: 0.01556101455309801\n",
      "attemp 124: loss: 0.01555700521232883\n",
      "attemp 125: loss: 0.01555300394341529\n",
      "attemp 126: loss: 0.015549010728065017\n",
      "attemp 127: loss: 0.015545025548031808\n",
      "attemp 128: loss: 0.015541048385115663\n",
      "attemp 129: loss: 0.01553707922116249\n",
      "attemp 130: loss: 0.015533118038063882\n",
      "attemp 131: loss: 0.015529164817757369\n",
      "attemp 132: loss: 0.015525219542225825\n",
      "attemp 133: loss: 0.01552128219349778\n",
      "attemp 134: loss: 0.015517352753646874\n",
      "attemp 135: loss: 0.01551343120479217\n",
      "attemp 136: loss: 0.0155095175290976\n",
      "attemp 137: loss: 0.015505611708772205\n",
      "attemp 138: loss: 0.01550171372606978\n",
      "attemp 139: loss: 0.015497823563288781\n",
      "attemp 140: loss: 0.01549394120277242\n",
      "attemp 141: loss: 0.015490066626908194\n",
      "attemp 142: loss: 0.015486199818128031\n",
      "attemp 143: loss: 0.015482340758908045\n",
      "attemp 144: loss: 0.01547848943176846\n",
      "attemp 145: loss: 0.015474645819273485\n",
      "attemp 146: loss: 0.015470809904031196\n",
      "attemp 147: loss: 0.015466981668693393\n",
      "attemp 148: loss: 0.015463161095955443\n",
      "attemp 149: loss: 0.015459348168556352\n",
      "attemp 150: loss: 0.015455542869278359\n",
      "attemp 151: loss: 0.015451745180947073\n",
      "attemp 152: loss: 0.015447955086431191\n",
      "attemp 153: loss: 0.01544417256864241\n",
      "attemp 154: loss: 0.01544039761053546\n",
      "attemp 155: loss: 0.015436630195107728\n",
      "attemp 156: loss: 0.01543287030539935\n",
      "attemp 157: loss: 0.015429117924492999\n",
      "attemp 158: loss: 0.015425373035513753\n",
      "attemp 159: loss: 0.015421635621629147\n",
      "attemp 160: loss: 0.015417905666048736\n",
      "attemp 161: loss: 0.01541418315202436\n",
      "attemp 162: loss: 0.015410468062849703\n",
      "attemp 163: loss: 0.015406760381860408\n",
      "attemp 164: loss: 0.015403060092433828\n",
      "attemp 165: loss: 0.015399367177988937\n",
      "attemp 166: loss: 0.015395681621986332\n",
      "attemp 167: loss: 0.015392003407927833\n",
      "attemp 168: loss: 0.015388332519356781\n",
      "attemp 169: loss: 0.015384668939857581\n",
      "attemp 170: loss: 0.015381012653055741\n",
      "attemp 171: loss: 0.015377363642617728\n",
      "attemp 172: loss: 0.015373721892250844\n",
      "attemp 173: loss: 0.015370087385703172\n",
      "attemp 174: loss: 0.015366460106763402\n",
      "attemp 175: loss: 0.015362840039260781\n",
      "attemp 176: loss: 0.015359227167064888\n",
      "attemp 177: loss: 0.015355621474085689\n",
      "attemp 178: loss: 0.01535202294427328\n",
      "attemp 179: loss: 0.015348431561617847\n",
      "attemp 180: loss: 0.015344847310149564\n",
      "attemp 181: loss: 0.015341270173938512\n",
      "attemp 182: loss: 0.01533770013709443\n",
      "attemp 183: loss: 0.015334137183766776\n",
      "attemp 184: loss: 0.01533058129814449\n",
      "attemp 185: loss: 0.01532703246445605\n",
      "attemp 186: loss: 0.015323490666969175\n",
      "attemp 187: loss: 0.01531995588999073\n",
      "attemp 188: loss: 0.015316428117866888\n",
      "attemp 189: loss: 0.015312907334982685\n",
      "attemp 190: loss: 0.015309393525762086\n",
      "attemp 191: loss: 0.01530588667466785\n",
      "attemp 192: loss: 0.015302386766201444\n",
      "attemp 193: loss: 0.015298893784902876\n",
      "attemp 194: loss: 0.015295407715350717\n",
      "attemp 195: loss: 0.015291928542161817\n",
      "attemp 196: loss: 0.015288456249991388\n",
      "attemp 197: loss: 0.015284990823532697\n",
      "attemp 198: loss: 0.015281532247517216\n",
      "attemp 199: loss: 0.01527808050671421\n",
      "attemp 200: loss: 0.015274635585930974\n",
      "attemp 201: loss: 0.015271197470012482\n",
      "attemp 202: loss: 0.015267766143841276\n",
      "attemp 203: loss: 0.015264341592337602\n",
      "attemp 204: loss: 0.015260923800459066\n",
      "attemp 205: loss: 0.015257512753200649\n",
      "attemp 206: loss: 0.015254108435594537\n",
      "attemp 207: loss: 0.01525071083271012\n",
      "attemp 208: loss: 0.015247319929653787\n",
      "attemp 209: loss: 0.015243935711568946\n",
      "attemp 210: loss: 0.01524055816363575\n",
      "attemp 211: loss: 0.015237187271071167\n",
      "attemp 212: loss: 0.015233823019128782\n",
      "attemp 213: loss: 0.015230465393098734\n",
      "attemp 214: loss: 0.015227114378307614\n",
      "attemp 215: loss: 0.015223769960118375\n",
      "attemp 216: loss: 0.015220432123930212\n",
      "attemp 217: loss: 0.015217100855178462\n",
      "attemp 218: loss: 0.015213776139334514\n",
      "attemp 219: loss: 0.015210457961905718\n",
      "attemp 220: loss: 0.01520714630843532\n",
      "attemp 221: loss: 0.015203841164502328\n",
      "attemp 222: loss: 0.015200542515721348\n",
      "attemp 223: loss: 0.015197250347742608\n",
      "attemp 224: loss: 0.015193964646251864\n",
      "attemp 225: loss: 0.015190685396970136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attemp 226: loss: 0.015187412585653834\n",
      "attemp 227: loss: 0.015184146198094502\n",
      "attemp 228: loss: 0.015180886220118784\n",
      "attemp 229: loss: 0.01517763263758833\n",
      "attemp 230: loss: 0.015174385436399688\n",
      "attemp 231: loss: 0.015171144602484258\n",
      "attemp 232: loss: 0.01516791012180808\n",
      "attemp 233: loss: 0.015164681980371906\n",
      "attemp 234: loss: 0.015161460164210973\n",
      "attemp 235: loss: 0.015158244659394896\n",
      "attemp 236: loss: 0.01515503545202775\n",
      "attemp 237: loss: 0.015151832528247842\n",
      "attemp 238: loss: 0.015148635874227566\n",
      "attemp 239: loss: 0.015145445476173403\n",
      "attemp 240: loss: 0.015142261320325843\n",
      "attemp 241: loss: 0.015139083392959265\n",
      "attemp 242: loss: 0.015135911680381845\n",
      "attemp 243: loss: 0.0151327461689354\n",
      "attemp 244: loss: 0.015129586844995463\n",
      "attemp 245: loss: 0.015126433694970945\n",
      "attemp 246: loss: 0.015123286705304322\n",
      "attemp 247: loss: 0.015120145862471377\n",
      "attemp 248: loss: 0.015117011152981079\n",
      "attemp 249: loss: 0.01511388256337568\n",
      "attemp 250: loss: 0.01511076008023038\n",
      "attemp 251: loss: 0.015107643690153454\n",
      "attemp 252: loss: 0.015104533379785984\n",
      "attemp 253: loss: 0.01510142913580197\n",
      "attemp 254: loss: 0.015098330944908067\n",
      "attemp 255: loss: 0.015095238793843549\n",
      "attemp 256: loss: 0.015092152669380283\n",
      "attemp 257: loss: 0.015089072558322519\n",
      "attemp 258: loss: 0.015085998447506974\n",
      "attemp 259: loss: 0.015082930323802562\n",
      "attemp 260: loss: 0.015079868174110472\n",
      "attemp 261: loss: 0.015076811985363899\n",
      "attemp 262: loss: 0.015073761744528152\n",
      "attemp 263: loss: 0.01507071743860044\n",
      "attemp 264: loss: 0.015067679054609845\n",
      "attemp 265: loss: 0.015064646579617173\n",
      "attemp 266: loss: 0.015061620000714985\n",
      "attemp 267: loss: 0.01505859930502735\n",
      "attemp 268: loss: 0.015055584479709906\n",
      "attemp 269: loss: 0.0150525755119497\n",
      "attemp 270: loss: 0.01504957238896515\n",
      "attemp 271: loss: 0.01504657509800589\n",
      "attemp 272: loss: 0.015043583626352745\n",
      "attemp 273: loss: 0.015040597961317664\n",
      "attemp 274: loss: 0.01503761809024355\n",
      "attemp 275: loss: 0.015034644000504275\n",
      "attemp 276: loss: 0.015031675679504504\n",
      "attemp 277: loss: 0.015028713114679787\n",
      "attemp 278: loss: 0.015025756293496175\n",
      "attemp 279: loss: 0.015022805203450436\n",
      "attemp 280: loss: 0.015019859832069835\n",
      "attemp 281: loss: 0.01501692016691202\n",
      "attemp 282: loss: 0.01501398619556508\n",
      "attemp 283: loss: 0.015011057905647262\n",
      "attemp 284: loss: 0.015008135284807109\n",
      "attemp 285: loss: 0.01500521832072319\n",
      "attemp 286: loss: 0.01500230700110415\n",
      "attemp 287: loss: 0.014999401313688581\n",
      "attemp 288: loss: 0.014996501246244937\n",
      "attemp 289: loss: 0.014993606786571417\n",
      "attemp 290: loss: 0.014990717922495988\n",
      "attemp 291: loss: 0.014987834641876254\n",
      "attemp 292: loss: 0.014984956932599295\n",
      "attemp 293: loss: 0.01498208478258175\n",
      "attemp 294: loss: 0.014979218179769604\n",
      "attemp 295: loss: 0.01497635711213817\n",
      "attemp 296: loss: 0.014973501567691958\n",
      "attemp 297: loss: 0.014970651534464684\n",
      "attemp 298: loss: 0.014967807000519168\n",
      "attemp 299: loss: 0.01496496795394712\n",
      "attemp 300: loss: 0.014962134382869353\n",
      "attemp 301: loss: 0.014959306275435342\n",
      "attemp 302: loss: 0.014956483619823466\n",
      "attemp 303: loss: 0.01495366640424076\n",
      "attemp 304: loss: 0.014950854616922842\n",
      "attemp 305: loss: 0.014948048246133931\n",
      "attemp 306: loss: 0.014945247280166664\n",
      "attemp 307: loss: 0.014942451707342088\n",
      "attemp 308: loss: 0.014939661516009582\n",
      "attemp 309: loss: 0.01493687669454679\n",
      "attemp 310: loss: 0.0149340972313594\n",
      "attemp 311: loss: 0.01493132311488135\n",
      "attemp 312: loss: 0.014928554333574477\n",
      "attemp 313: loss: 0.014925790875928614\n",
      "attemp 314: loss: 0.01492303273046142\n",
      "attemp 315: loss: 0.014920279885718405\n",
      "attemp 316: loss: 0.014917532330272779\n",
      "attemp 317: loss: 0.014914790052725306\n",
      "attemp 318: loss: 0.014912053041704464\n",
      "attemp 319: loss: 0.014909321285866162\n",
      "attemp 320: loss: 0.014906594773893738\n",
      "attemp 321: loss: 0.014903873494497817\n",
      "attemp 322: loss: 0.01490115743641647\n",
      "attemp 323: loss: 0.014898446588414821\n",
      "attemp 324: loss: 0.014895740939285185\n",
      "attemp 325: loss: 0.014893040477846927\n",
      "attemp 326: loss: 0.014890345192946427\n",
      "attemp 327: loss: 0.014887655073457\n",
      "attemp 328: loss: 0.014884970108278755\n",
      "attemp 329: loss: 0.01488229028633857\n",
      "attemp 330: loss: 0.014879615596590127\n",
      "attemp 331: loss: 0.014876946028013678\n",
      "attemp 332: loss: 0.014874281569616028\n",
      "attemp 333: loss: 0.014871622210430454\n",
      "attemp 334: loss: 0.014868967939516767\n",
      "attemp 335: loss: 0.014866318745961036\n",
      "attemp 336: loss: 0.014863674618875585\n",
      "attemp 337: loss: 0.014861035547399098\n",
      "attemp 338: loss: 0.014858401520696273\n",
      "attemp 339: loss: 0.014855772527957932\n",
      "attemp 340: loss: 0.014853148558400907\n",
      "attemp 341: loss: 0.01485052960126795\n",
      "attemp 342: loss: 0.014847915645827676\n",
      "attemp 343: loss: 0.014845306681374567\n",
      "attemp 344: loss: 0.014842702697228764\n",
      "attemp 345: loss: 0.014840103682736116\n",
      "attemp 346: loss: 0.014837509627268035\n",
      "attemp 347: loss: 0.014834920520221494\n",
      "attemp 348: loss: 0.014832336351018924\n",
      "attemp 349: loss: 0.014829757109108152\n",
      "attemp 350: loss: 0.01482718278396228\n",
      "attemp 351: loss: 0.01482461336507978\n",
      "attemp 352: loss: 0.014822048841984195\n",
      "attemp 353: loss: 0.014819489204224325\n",
      "attemp 354: loss: 0.014816934441373917\n",
      "attemp 355: loss: 0.01481438454303177\n",
      "attemp 356: loss: 0.014811839498821576\n",
      "attemp 357: loss: 0.014809299298391953\n",
      "attemp 358: loss: 0.014806763931416295\n",
      "attemp 359: loss: 0.014804233387592687\n",
      "attemp 360: loss: 0.014801707656643901\n",
      "attemp 361: loss: 0.014799186728317352\n",
      "attemp 362: loss: 0.014796670592384958\n",
      "attemp 363: loss: 0.014794159238643068\n",
      "attemp 364: loss: 0.014791652656912551\n",
      "attemp 365: loss: 0.014789150837038481\n",
      "attemp 366: loss: 0.014786653768890377\n",
      "attemp 367: loss: 0.014784161442361794\n",
      "attemp 368: loss: 0.014781673847370603\n",
      "attemp 369: loss: 0.014779190973858611\n",
      "attemp 370: loss: 0.014776712811791812\n",
      "attemp 371: loss: 0.01477423935116002\n",
      "attemp 372: loss: 0.014771770581977059\n",
      "attemp 373: loss: 0.014769306494280486\n",
      "attemp 374: loss: 0.014766847078131713\n",
      "attemp 375: loss: 0.014764392323615821\n",
      "attemp 376: loss: 0.014761942220841524\n",
      "attemp 377: loss: 0.014759496759941234\n",
      "attemp 378: loss: 0.014757055931070745\n",
      "attemp 379: loss: 0.014754619724409371\n",
      "attemp 380: loss: 0.014752188130159872\n",
      "attemp 381: loss: 0.014749761138548262\n",
      "attemp 382: loss: 0.014747338739823899\n",
      "attemp 383: loss: 0.014744920924259322\n",
      "attemp 384: loss: 0.014742507682150257\n",
      "attemp 385: loss: 0.014740099003815507\n",
      "attemp 386: loss: 0.0147376948795969\n",
      "attemp 387: loss: 0.014735295299859251\n",
      "attemp 388: loss: 0.014732900254990322\n",
      "attemp 389: loss: 0.014730509735400684\n",
      "attemp 390: loss: 0.014728123731523685\n",
      "attemp 391: loss: 0.014725742233815476\n",
      "attemp 392: loss: 0.014723365232754845\n",
      "attemp 393: loss: 0.014720992718843164\n",
      "attemp 394: loss: 0.0147186246826044\n",
      "attemp 395: loss: 0.01471626111458502\n",
      "attemp 396: loss: 0.014713902005353946\n",
      "attemp 397: loss: 0.014711547345502406\n",
      "attemp 398: loss: 0.014709197125644013\n",
      "attemp 399: loss: 0.014706851336414653\n",
      "attemp 400: loss: 0.014704509968472336\n",
      "attemp 401: loss: 0.014702173012497318\n",
      "attemp 402: loss: 0.014699840459191855\n",
      "attemp 403: loss: 0.0146975122992803\n",
      "attemp 404: loss: 0.014695188523508929\n",
      "attemp 405: loss: 0.014692869122646\n",
      "attemp 406: loss: 0.014690554087481544\n",
      "attemp 407: loss: 0.014688243408827474\n",
      "attemp 408: loss: 0.014685937077517334\n",
      "attemp 409: loss: 0.014683635084406552\n",
      "attemp 410: loss: 0.014681337420371964\n",
      "attemp 411: loss: 0.014679044076312094\n",
      "attemp 412: loss: 0.014676755043146979\n",
      "attemp 413: loss: 0.014674470311818124\n",
      "attemp 414: loss: 0.014672189873288393\n",
      "attemp 415: loss: 0.014669913718542066\n",
      "attemp 416: loss: 0.014667641838584664\n",
      "attemp 417: loss: 0.014665374224442944\n",
      "attemp 418: loss: 0.014663110867164849\n",
      "attemp 419: loss: 0.014660851757819535\n",
      "attemp 420: loss: 0.014658596887497101\n",
      "attemp 421: loss: 0.014656346247308742\n",
      "attemp 422: loss: 0.01465409982838658\n",
      "attemp 423: loss: 0.014651857621883695\n",
      "attemp 424: loss: 0.01464961961897397\n",
      "attemp 425: loss: 0.014647385810852098\n",
      "attemp 426: loss: 0.014645156188733535\n",
      "attemp 427: loss: 0.014642930743854416\n",
      "attemp 428: loss: 0.014640709467471518\n",
      "attemp 429: loss: 0.01463849235086218\n",
      "attemp 430: loss: 0.014636279385324318\n",
      "attemp 431: loss: 0.014634070562176244\n",
      "attemp 432: loss: 0.014631865872756802\n",
      "attemp 433: loss: 0.01462966530842515\n",
      "attemp 434: loss: 0.014627468860560693\n",
      "attemp 435: loss: 0.014625276520563247\n",
      "attemp 436: loss: 0.014623088279852717\n",
      "attemp 437: loss: 0.01462090412986925\n",
      "attemp 438: loss: 0.014618724062073042\n",
      "attemp 439: loss: 0.014616548067944355\n",
      "attemp 440: loss: 0.0146143761389835\n",
      "attemp 441: loss: 0.01461220826671067\n",
      "attemp 442: loss: 0.014610044442665985\n",
      "attemp 443: loss: 0.014607884658409436\n",
      "attemp 444: loss: 0.014605728905520778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attemp 445: loss: 0.014603577175599524\n",
      "attemp 446: loss: 0.014601429460264876\n",
      "attemp 447: loss: 0.01459928575115568\n",
      "attemp 448: loss: 0.014597146039930389\n",
      "attemp 449: loss: 0.014595010318266944\n",
      "attemp 450: loss: 0.014592878577862827\n",
      "attemp 451: loss: 0.014590750810434965\n",
      "attemp 452: loss: 0.014588627007719627\n",
      "attemp 453: loss: 0.014586507161472475\n",
      "attemp 454: loss: 0.014584391263468392\n",
      "attemp 455: loss: 0.014582279305501548\n",
      "attemp 456: loss: 0.01458017127938533\n",
      "attemp 457: loss: 0.014578067176952153\n",
      "attemp 458: loss: 0.014575966990053636\n",
      "attemp 459: loss: 0.014573870710560405\n",
      "attemp 460: loss: 0.014571778330362044\n",
      "attemp 461: loss: 0.014569689841367113\n",
      "attemp 462: loss: 0.014567605235503048\n",
      "attemp 463: loss: 0.014565524504716114\n",
      "attemp 464: loss: 0.014563447640971424\n",
      "attemp 465: loss: 0.014561374636252794\n",
      "attemp 466: loss: 0.014559305482562767\n",
      "attemp 467: loss: 0.014557240171922463\n",
      "attemp 468: loss: 0.01455517869637169\n",
      "attemp 469: loss: 0.014553121047968787\n",
      "attemp 470: loss: 0.014551067218790539\n",
      "attemp 471: loss: 0.01454901720093231\n",
      "attemp 472: loss: 0.014546970986507762\n",
      "attemp 473: loss: 0.014544928567648942\n",
      "attemp 474: loss: 0.01454288993650627\n",
      "attemp 475: loss: 0.014540855085248359\n",
      "attemp 476: loss: 0.014538824006062082\n",
      "attemp 477: loss: 0.014536796691152496\n",
      "attemp 478: loss: 0.014534773132742816\n",
      "attemp 479: loss: 0.014532753323074188\n",
      "attemp 480: loss: 0.014530737254405962\n",
      "attemp 481: loss: 0.01452872491901538\n",
      "attemp 482: loss: 0.014526716309197678\n",
      "attemp 483: loss: 0.014524711417265955\n",
      "attemp 484: loss: 0.014522710235551151\n",
      "attemp 485: loss: 0.01452071275640204\n",
      "attemp 486: loss: 0.014518718972185134\n",
      "attemp 487: loss: 0.014516728875284646\n",
      "attemp 488: loss: 0.014514742458102494\n",
      "attemp 489: loss: 0.014512759713058149\n",
      "attemp 490: loss: 0.014510780632588703\n",
      "attemp 491: loss: 0.014508805209148788\n",
      "attemp 492: loss: 0.014506833435210538\n",
      "attemp 493: loss: 0.014504865303263437\n",
      "attemp 494: loss: 0.014502900805814428\n",
      "attemp 495: loss: 0.01450093993538781\n",
      "attemp 496: loss: 0.014498982684525156\n",
      "attemp 497: loss: 0.014497029045785322\n",
      "attemp 498: loss: 0.014495079011744412\n",
      "attemp 499: loss: 0.01449313257499559\n",
      "attemp 500: loss: 0.014491189728149318\n",
      "attemp 501: loss: 0.014489250463832981\n",
      "attemp 502: loss: 0.014487314774691087\n",
      "attemp 503: loss: 0.014485382653385118\n",
      "attemp 504: loss: 0.014483454092593566\n",
      "attemp 505: loss: 0.014481529085011715\n",
      "attemp 506: loss: 0.014479607623351836\n",
      "attemp 507: loss: 0.01447768970034296\n",
      "attemp 508: loss: 0.014475775308730909\n",
      "attemp 509: loss: 0.01447386444127822\n",
      "attemp 510: loss: 0.014471957090764188\n",
      "attemp 511: loss: 0.014470053249984682\n",
      "attemp 512: loss: 0.014468152911752232\n",
      "attemp 513: loss: 0.014466256068895964\n",
      "attemp 514: loss: 0.014464362714261403\n",
      "attemp 515: loss: 0.014462472840710682\n",
      "attemp 516: loss: 0.014460586441122355\n",
      "attemp 517: loss: 0.014458703508391294\n",
      "attemp 518: loss: 0.014456824035428808\n",
      "attemp 519: loss: 0.014454948015162469\n",
      "attemp 520: loss: 0.014453075440536162\n",
      "attemp 521: loss: 0.014451206304510012\n",
      "attemp 522: loss: 0.014449340600060246\n",
      "attemp 523: loss: 0.014447478320179303\n",
      "attemp 524: loss: 0.01444561945787579\n",
      "attemp 525: loss: 0.014443764006174217\n",
      "attemp 526: loss: 0.014441911958115268\n",
      "attemp 527: loss: 0.014440063306755572\n",
      "attemp 528: loss: 0.014438218045167652\n",
      "attemp 529: loss: 0.01443637616643996\n",
      "attemp 530: loss: 0.014434537663676823\n",
      "attemp 531: loss: 0.014432702529998384\n",
      "attemp 532: loss: 0.014430870758540569\n",
      "attemp 533: loss: 0.014429042342455026\n",
      "attemp 534: loss: 0.014427217274909088\n",
      "attemp 535: loss: 0.014425395549085833\n",
      "attemp 536: loss: 0.014423577158183843\n",
      "attemp 537: loss: 0.014421762095417371\n",
      "attemp 538: loss: 0.014419950354016192\n",
      "attemp 539: loss: 0.014418141927225528\n",
      "attemp 540: loss: 0.01441633680830611\n",
      "attemp 541: loss: 0.014414534990534142\n",
      "attemp 542: loss: 0.01441273646720106\n",
      "attemp 543: loss: 0.014410941231613798\n",
      "attemp 544: loss: 0.01440914927709453\n",
      "attemp 545: loss: 0.014407360596980672\n",
      "attemp 546: loss: 0.014405575184624908\n",
      "attemp 547: loss: 0.01440379303339511\n",
      "attemp 548: loss: 0.01440201413667429\n",
      "attemp 549: loss: 0.014400238487860534\n",
      "attemp 550: loss: 0.014398466080367062\n",
      "attemp 551: loss: 0.014396696907622077\n",
      "attemp 552: loss: 0.014394930963068842\n",
      "attemp 553: loss: 0.014393168240165522\n",
      "attemp 554: loss: 0.014391408732385227\n",
      "attemp 555: loss: 0.014389652433215907\n",
      "attemp 556: loss: 0.01438789933616044\n",
      "attemp 557: loss: 0.014386149434736433\n",
      "attemp 558: loss: 0.014384402722476314\n",
      "attemp 559: loss: 0.0143826591929272\n",
      "attemp 560: loss: 0.014380918839650947\n",
      "attemp 561: loss: 0.014379181656224054\n",
      "attemp 562: loss: 0.01437744763623758\n",
      "attemp 563: loss: 0.014375716773297314\n",
      "attemp 564: loss: 0.01437398906102339\n",
      "attemp 565: loss: 0.014372264493050657\n",
      "attemp 566: loss: 0.014370543063028277\n",
      "attemp 567: loss: 0.014368824764619917\n",
      "attemp 568: loss: 0.014367109591503679\n"
     ]
    }
   ],
   "source": [
    "epoch=1000\n",
    "learning_rate=0.005\n",
    "for i in range(0,epoch):\n",
    "    print(\"attemp \"+str(i)+\": loss: \"+str(computeLoss(sof,lor,gpa,y)))\n",
    "#     print(\"attemp \"+str(i)+\": a: \"+str(a))\n",
    "#     print(\"attemp \"+str(i)+\": b: \"+str(b))\n",
    "#     print(\"attemp \"+str(i)+\": c: \"+str(c))\n",
    "#     print(\"attemp \"+str(i)+\": d: \"+str(d))\n",
    "#     print(\"attemp \"+str(i)+\": e: \"+str(e))\n",
    "#     print(\"attemp \"+str(i)+\": f: \"+str(f))\n",
    "#     print(\"attemp \"+str(i)+\": g: \"+str(g))\n",
    "#     print(\"attemp \"+str(i)+\": h: \"+str(h))\n",
    "#     print(\"attemp \"+str(i)+\": k: \"+str(k))\n",
    "#     print(\"attemp \"+str(i)+\": l: \"+str(l))\n",
    "#     print(\"attemp \"+str(i)+\": m: \"+str(m))\n",
    "    a-=learning_rate*computeGrad(sof,lor,gpa,y,\"a\")\n",
    "    b-=learning_rate*computeGrad(sof,lor,gpa,y,\"b\")\n",
    "    c-=learning_rate*computeGrad(sof,lor,gpa,y,\"c\")\n",
    "    d-=learning_rate*computeGrad(sof,lor,gpa,y,\"d\")\n",
    "    e-=learning_rate*computeGrad(sof,lor,gpa,y,\"e\")\n",
    "    f-=learning_rate*computeGrad(sof,lor,gpa,y,\"f\")\n",
    "    g-=learning_rate*computeGrad(sof,lor,gpa,y,\"g\")\n",
    "    h-=learning_rate*computeGrad(sof,lor,gpa,y,\"h\")\n",
    "    k-=learning_rate*computeGrad(sof,lor,gpa,y,\"k\")\n",
    "    l-=learning_rate*computeGrad(sof,lor,gpa,y,\"l\")\n",
    "    m-=learning_rate*computeGrad(sof,lor,gpa,y,\"m\")\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def know_your_chance(GRE,Uni_s, CGPA):\n",
    "    chance=(a*GRE+b*Uni_s+c*CGPA+d)*m+(e*GRE+f*Uni_s+g*CGPA+h)*k+l\n",
    "    print(\"Your chance of getting accepted is: \",int(chance*100),\"%\",sep=\"\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "know_your_chance(4,4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
