{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beegii/image_processing_dir/image_processing_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/beegii/image_processing_dir/image_processing_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/beegii/image_processing_dir/image_processing_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/beegii/image_processing_dir/image_processing_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/beegii/image_processing_dir/image_processing_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/beegii/image_processing_dir/image_processing_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides = 1):\n",
    "    x = tf.nn.conv2d(x, W, strides = [1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize = [1, k, k, 1], strides = [1, k, k, 1], padding = 'SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(x, weight, bias, dropout):\n",
    "    x = tf.reshape(x, shape = [-1, 28, 28, 1])\n",
    "    \n",
    "    conv1 = conv2d(x, weight['w1'], bias['b1']) # Convolutional Layer 1\n",
    "    conv1 = maxpool2d(conv1) # Pooling Layer 1\n",
    "    \n",
    "    conv2 = conv2d(conv1, weight['w2'], bias['b2']) # Convolutional Layer 1\n",
    "    conv2 = maxpool2d(conv2) # Pooling Layer 1\n",
    "    \n",
    "    # Fully Connected Layer 1\n",
    "    # Reshaping output of previous convolutional layer to fit the fully connected layer\n",
    "    fc = tf.reshape(conv2, [-1, weights['w3'].get_shape().as_list()[0]])\n",
    "    fc = tf.add(tf.matmul(fc, weight['w3']), bias['b3']) # Linear Function\n",
    "    fc = tf.nn.relu(fc) # Activation Function\n",
    "    \n",
    "    fc = tf.nn.dropout(fc, dropout) # Applying dropout on Fully Connected Layer\n",
    "    \n",
    "    out = tf.add(tf.matmul(fc, weight['w4']), bias['b4']) # Output Layer\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = tf.placeholder(tf.float32, shape = [None, 784]) # Placeholder for Feature Matrix\n",
    "Y2 = tf.placeholder(tf.float32, shape = [None, 6]) # Placeholder for Labels\n",
    "keep_prob = tf.placeholder(tf.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'w1.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-31d4df642218>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m weights = {\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m'w1'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'w1.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m'w2'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'w2.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'w3'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'w3.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'w4'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'w4.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/image_processing_dir/image_processing_env/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'w1.npy'"
     ]
    }
   ],
   "source": [
    "weights = {\n",
    "    'w1' : tf.convert_to_tensor(np.array(np.load('w1.npy')),dtype=tf.float32),\n",
    "    'w2' : tf.convert_to_tensor(np.array(np.load('w2.npy')),dtype=tf.float32),\n",
    "    'w3' : tf.convert_to_tensor(np.array(np.load('w3.npy')),dtype=tf.float32),\n",
    "    'w4' : tf.convert_to_tensor(np.array(np.load('w4.npy')),dtype=tf.float32)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'b1.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-480fcd8814af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m biases = {\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m'b1'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'b1.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m'b2'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'b2.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'b3'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'b3.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'b4'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'b4.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/image_processing_dir/image_processing_env/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'b1.npy'"
     ]
    }
   ],
   "source": [
    "biases = {\n",
    "    'b1' : tf.convert_to_tensor(np.array(np.load('b1.npy')),dtype=tf.float32),\n",
    "    'b2' : tf.convert_to_tensor(np.array(np.load('b2.npy')),dtype=tf.float32),\n",
    "    'b3' : tf.convert_to_tensor(np.array(np.load('b3.npy')),dtype=tf.float32),\n",
    "    'b4' : tf.convert_to_tensor(np.array(np.load('b4.npy')),dtype=tf.float32)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = neural_network(X1, weights, biases, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(img):\n",
    "    with tf.Session() as sess:\n",
    "        pred = sess.run(y_pred, feed_dict = { X1 : img, keep_prob : 1.0 })\n",
    "    img = img.reshape(28, 28)\n",
    "    pred = list(pred.flatten())\n",
    "    pred = pred.index(max(pred))*20\n",
    "    return (img, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"test/img1.jpg\",0)\n",
    "img = cv2.resize(img,(28,28))\n",
    "img=img.flatten()\n",
    "img = torch.tensor(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "[0, 5, 4, 5, 4, 4, 3, 1, 1, 5, 2, 0, 3, 4, 0, 5, 4]\n",
      "torch.Size([17, 784])\n",
      "torch.Size([17])\n"
     ]
    }
   ],
   "source": [
    "with open('test.json') as g:\n",
    "    datatest = json.load(g)\n",
    "    \n",
    "size = 28\n",
    "ldata = []\n",
    "ydata = []\n",
    "for zurag in datatest['test']:\n",
    "    img = cv2.imread(\"test/\"+zurag['name'],0)\n",
    "    \n",
    "    img = cv2.resize(img,(size,size))\n",
    "  \n",
    "    ldata.append(img.flatten())\n",
    "    ydata.append(int(zurag['smile']))\n",
    "print(len(ldata))\n",
    "print(ydata)\n",
    "X = torch.tensor(ldata)\n",
    "Y = torch.tensor(ydata)\n",
    "test_x = X\n",
    "\n",
    "test_y = Y\n",
    "\n",
    "\n",
    "print(test_x.shape)\n",
    "\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-36522126da14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_prediction' is not defined"
     ]
    }
   ],
   "source": [
    "image, pred = get_prediction(img.reshape(1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image, cmap = 'binary')\n",
    "plt.title(pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
